{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch and pyro; set random generator seed\n",
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "# to clear all variables stored by Pyro\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# enable validation for useful warnings and errors\n",
    "pyro.enable_validation(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Introduction to inference\n",
    "\n",
    "In pyro, the model function can be used for approximate inference. Here, we will prepare another simple example and use it for inference.\n",
    "\n",
    "For example, imagine driving a car with an inaccurate speedometer, that gives different values ever, let's say, with a standard deviation of 5.0. We could try to guess the speed based on how the surroundings move and use this guess to compensate for the inaccuracy of the speedometer, however our guess is inaccurate as well. This is shown by this model:\n",
    "\n",
    "$$speed \\mid guess \\sim \\mathcal{N}(guess, 10^2)$$\n",
    "$$measurement \\mid speed \\sim \\mathcal{N}(speed, 5^2)$$\n",
    "\n",
    "Additionally we want to be able to pass multiple guesses and obtain measurement for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speedometer(guess):\n",
    "    speed = pyro.sample(\"speed\", dist.Normal(guess, 10.))\n",
    "    with pyro.plate(\"data\", len(guess)) as ind:\n",
    "        return pyro.sample(\"measurement\", dist.Normal(speed, 5.))\n",
    "    \n",
    "\n",
    "guesses = torch.Tensor([40, 42])\n",
    "for i in range(5):\n",
    "    print(speedometer(guesses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyro.plate`\n",
    "[`pyro.plate`](http://docs.pyro.ai/en/stable/primitives.html#pyro.primitives.plate) is a construct for conditionally independent sequences of variables.\n",
    "\n",
    "Sequential `plate` is similar to `range()` in that it generates a sequence of values.\n",
    "\n",
    "Vectorized `plate` is similar to `torch.arange()` in that it yields an array of indices by which other tensors can be indexed.\n",
    "\n",
    "`plate` also informs inference algorithms that the variables being indexed are conditionally independent. To do this, `plate` is a provided as context manager rather than a function, and **users must guarantee that all computation within an plate context is conditionally independent.**\n",
    "\n",
    "`plate` can be used for in a subsampling mode and can be assigned to a `device`. For more details check the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning\n",
    "\n",
    "The utility of probabilistic programming is the ability to **condition** the generative models (such as the `speedometer`) on observed data and infer the latent factors, that might have produced the data.\n",
    "\n",
    "In Pyro, the expression of condition is separated from evaluation via inference, so that we can write a model once and condition it on many different observations. We can constrain a model's internal `sample` statements to be equal to a given set of observations.\n",
    "\n",
    "Considering our `speedometer`, we want to sample from the distribution of `speed` given some input `guess`, and observing that `measurement = 41`; we are trying to infer the distribution\n",
    "\n",
    "$$\\left(speed \\mid guess, measurement = 41\\right) \\sim ?$$\n",
    "\n",
    "That's where `pyro.condition` comes handy. It allows us to constrain the values of sample statements. In other words, `pyro.condition` means to force the sample to return the provided value. It is a higher-order function that takes a model and a dictionary of observations and returns a new model, that has the same input and output signatures, but always uses the given value at observed `sample` statements. We can check several quesses at once. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = torch.tensor([41.])\n",
    "guesses = torch.Tensor([40, 42])\n",
    "conditioned_speedometer = pyro.condition(speedometer, data={\"measurement\": measurements})\n",
    "for i in range(5):\n",
    "    print(conditioned_speedometer(guesses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sampling from such a conditional makes no sense - the `measurement` is rigidly set. We need a way to access the \"hidden\" states. The `Predictive` class from `pyro.infer` is a way to do it.\n",
    "\n",
    "By specyfing `return_sites` we restrict the `Predictive` to return only selected samples. By default it returns all sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import Predictive\n",
    "Predictive(conditioned_speedometer, num_samples=5, return_sites=('speed',))(guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioned model behaves like an ordinary Python function - it might be parametrized as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deferred_conditioned_speedometer(guess, measurement):\n",
    "    return pyro.condition(speedometer, data={\"measurement\": measurement})(guess)\n",
    "\n",
    "Predictive(deferred_conditioned_speedometer, num_samples=5, return_sites=('speed',))(guesses, measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `python.condition` we might also add the optional `obs` argument to `pyro.sample`. `obs` keyword is exclusive to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deferred_conditioned_speedometer(guess, measurement=None):\n",
    "    speed = pyro.sample(\"speed\", dist.Normal(guess, 10.))\n",
    "    with pyro.plate(\"data\", len(guess)) as ind:\n",
    "        return pyro.sample(\"measurement\", dist.Normal(speed, 5.), obs=measurement)\n",
    "\n",
    "print(deferred_conditioned_speedometer(guesses, measurements))\n",
    "print(Predictive(deferred_conditioned_speedometer, num_samples=5, return_sites=None)(guesses, measurements))\n",
    "\n",
    "print(deferred_conditioned_speedometer(guesses))\n",
    "print(Predictive(deferred_conditioned_speedometer, num_samples=5, return_sites=None)(guesses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `len(measurement) = 1` and the length of the model output is also 1 although `len(guesses) = 2`. It is convenient to provide `guesses` and `measurement`s of equal length or more general `input` and `output`, `x` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide functions\n",
    "\n",
    "With conditioning on an observation of `measurement` we are now able to use Pyro's approximate inference algorithm to estimate the distribution over `speed` given `guess` and `measurement == data`.\n",
    "\n",
    "That's where *guide functions* come in use. In Pyro, the model function can be treated as the real *posterior distribution* $P(speed \\mid guess, measurement)$ (given the `guess` and `measurement`, what is the probability distribution over `speed`), whereas guide is the approximation of the posterior distribution $Q(speed)$ (approximation of probability distribution over the latent variables) - the closest possible to `P`.\n",
    "\n",
    "In practice, $Q$ will be a simpler distribution than $P$, and in Pyro guide tries to approximate the model as good as it's possible.\n",
    "\n",
    "![image.png](https://miro.medium.com/max/800/1*cUMrUSWmLClDfYa9LTO8KA.png)\n",
    "\n",
    "There are two criterias for guide functions to be a valid approximations for a particular model:\n",
    "\n",
    "  *  all unobserved (i.e. not conditioned) sample statements that appear in the model must appear in the guide\n",
    "  *  the guide must have the same input signature as the model (i.e. take the same arguments)\n",
    "  \n",
    "The precise meaning of the guide is different across different inference algorithms (in Pyro importance sampling, MCMC and stochastic variational inference are available).\n",
    "\n",
    "Distributions (with it's parameters) set in the model are treated as priors!\n",
    "\n",
    "In the case of our `speedometer`, the true posterior distribution is actually tractable and could be determined analytically, however, such case is rather an exception than the rule. For example, in a case of a nonlinearity in the model the posterior may become intractable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_function(tensor):\n",
    "    mask = (tensor >= 10).float()\n",
    "    pos = mask * (0.01 * tensor ** 2 + tensor)\n",
    "    neg = (1 - mask) * torch.abs(0.02 * tensor ** 2 + 0.8 * tensor)\n",
    "    return pos + neg\n",
    "\n",
    "def intractable_speedometer(guess, obs=None):\n",
    "    speed = pyro.sample(\"speed\", dist.Normal(guess, 10.))\n",
    "    with pyro.plate(\"data\", len(guess)) as ind:\n",
    "        return pyro.sample(\"measurement\", dist.Normal(nonlinear_function(speed), 5), obs=obs)\n",
    "    \n",
    "intractable_speedometer(guesses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such case we should use a top-level function `pyro.param`. It specifies a *family* of guides indexed by named parameters, and searches for the member of that family that is the best approximation according to some loss functions. This approach to approximate posterior is called ***variational inference***.\n",
    "\n",
    "`pyro.param` is exclusive to guides. It is a frontend for Pyro's key-value *parameter store*. Like `pyro.sample`, it is always called with a name as its first argument.\n",
    "\n",
    "The first time `pyro.param` is called with a particular name, its argument is stored in the parameter store and the value is returned. After that, whent it's called with that name, it returns the value from the parameter store regardless of any other argument. Furthermore, `pyro.param` provides differentiation for the inputs - this are the pieces of the guide that are actually going to be trained.\n",
    "\n",
    "For example, we could parametrize normal distribution in `speedometer_guide` as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speedometer_parametrized_guide(guess, obs=None):  # same argument set as the model\n",
    "    scale = pyro.param(\"scale\", torch.tensor(1.))\n",
    "    pyro.sample(\"speed\", dist.Normal(guess, torch.abs(scale)))  # notice absolute value of scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that since variance of normal distribution has to be positive, we applied `torch.abs`. There are other restrictions that might be necessary in other distributions. PyTorch distributions library includes a module containing such restrictions: [`torch.distributions.constraints`](https://pytorch.org/docs/master/distributions.html#module-torch.distributions.constraints). Applying a constrinat is done by passing the relevant object to `pyro.param` as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "\n",
    "\n",
    "def speedometer_parametrized_guide_constrained(guess, obs=None):\n",
    "    scale = pyro.param(\"scale\", torch.tensor(1.), constraint=constraints.positive)\n",
    "    pyro.sample(\"speed\", dist.Normal(guess, scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic variational inference\n",
    "\n",
    "In Pyro, the main algorithm of variational inference is stochastic variational inference. It has three key characteristics:\n",
    "  *  parameters are real-valued tensors\n",
    "  *  estimates of a loss function are computed using Monte Carlo estimates from samples of execution histories of the model and guide\n",
    "  *  to search for optimal parameters, stochastic gradient descent is used\n",
    "\n",
    "Each step in the training proess will take one pass through the model and incorporate the observed data. Then, it will go through the model once again, replacing `pyro.sample` statements with corresponding statement from the guide and compare the resulting distribution from the model and the guide, to adjust the `pyro.param` values in order to get the guide closer to the model. In theory, that should cause the guide to follow the model and get close enough to approixmate it.\n",
    "\n",
    "The difference between two probability distribution is commonly quantified using the KL divergence:\n",
    "\n",
    "$$D_{KL}\\left(guide \\mid\\mid model\\right) = -\\sum_{x \\in \\mathcal{X}} Q_{guide}\\left(x\\right) \\left(\\log P_{model}\\left(x \\mid z\\right) - \\log Q_{guide}\\left(x\\right)\\right)$$\n",
    "$$=-\\mathbb{E}_{x \\sim Q_{guide}} \\left[\\log P_{model}\\left(x \\mid z\\right) - \\log Q_{guide}\\left(x\\right)\\right]$$\n",
    "\n",
    "> For all possible values of the latent variable(s) $x$, compute the difference in log probability between the model and the guide, and weigh that difference by the likelihood of each assignment under the guide (expectation of the difference in terms of the guide)\n",
    "\n",
    "This still requires us to know $P_{model}(z)$, which is usually intractable. Instead, we can factor out the intractable term to get ELBO (*Evidence Lower BOund*):\n",
    "\n",
    "$$ELBO\\left(guide \\mid model\\right) = \\mathbb{E}_{x \\sim Q_{guide}} \\left[\\log P_{model}\\left(x, z\\right) - \\log Q_{guide}\\left(x\\right)\\right]$$\n",
    "\n",
    "The crucial difference is, that here $P_{model}$ is a joint probability and therefore might be computed directly in the code.\n",
    "\n",
    "Our goal is to use the ELBO to determine how different the guide function is from the conditioned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = torch.tensor([40.])\n",
    "guesses = torch.tensor([41.])\n",
    "\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI(model=deferred_conditioned_speedometer,\n",
    "                     guide=speedometer_parametrized_guide_constrained,\n",
    "                     optim=pyro.optim.SGD({\"lr\": 1e-3}),\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "losses, scales = [], []\n",
    "n_steps = 5_000\n",
    "for t in trange(n_steps):\n",
    "    losses.append(svi.step(guesses, measurements))\n",
    "    scales.append(pyro.param(\"scale\").item())\n",
    "    \n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "print(f\"scale = {pyro.param('scale').item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('scale')\n",
    "plt.plot(scales)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that we will load the parameter values from the parameter store - that's where Pyro stores them throughout training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(pyro.get_param_store())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a guide for coin flipping model, with starting param value $p = 0.5$. Train a model, show a plot of how loss and predicted probability varies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_coin():\n",
    "    # copy from previous notebook\n",
    "    pass\n",
    "\n",
    "def flip_coin_guide():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_steps = 2500\n",
    "\n",
    "pyro.clear_param_store()\n",
    "svi = pyro.infer.SVI() # fill arguments\n",
    "\n",
    "losses, probas = [], []\n",
    "# write training loop\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"ELBO\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "print(f\"p = {pyro.param('p').item()}\")\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "plt.plot(probas)\n",
    "plt.ylabel('probability')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
