{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyro\n",
    "====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pyro** is a **probabilistic programming language** released by Uber AI Labs. It is a tool for deep probabilistic modelling built on top of PyTorch, unifying the best of modern deep learning and Bayesian modeling. [[link]](https://eng.uber.com/pyro/)\n",
    "\n",
    "A probabilistic programming language is a programming language designed to describe probabilistic models and then perform inference in those models.\n",
    "\n",
    "Like PyTorch, Pyro supports arbitrary Python code (iteration, recursion, higher-order functions etc.), which makes it universal to represent any computable probability distribution. Thanks to PyTorch, Pyro can benefit from GPU-accelerated tensor math.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch and pyro; set random generator seed\n",
    "import torch\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "pyro.set_rng_seed(101)\n",
    "\n",
    "# to clear all variables stored by Pyro\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# enable validation for useful warnings and errors\n",
    "pyro.enable_validation(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Primitive Stochastic Functions\n",
    "\n",
    "The key element of probabilistic programs are primitive stochastic functions, or distributions. Currently, Pyro uses PyTorch distributions: [`torch.distributions`](https://pytorch.org/docs/stable/distributions.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a sample from N(0, 1)\n",
    "loc = 0.  # mean\n",
    "scale = 1.  # variance\n",
    "normal_dist = torch.distributions.Normal(loc, scale)  # normal distribution N(0, 1)\n",
    "x = normal_dist.rsample()  # draw a sample - for N rsample\n",
    "print(f\"normal | sample: {x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a sample from B(alpha=2, beta=5)\n",
    "alpha = 2\n",
    "beta = 5\n",
    "beta_dist = torch.distributions.Beta(alpha, beta)\n",
    "x = beta_dist.sample()\n",
    "print(f\"beta | sample: {x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "In Pyro, models are Python functions, which take data as input and use Pyro primitives to analyze the data. All probabilistic programs are built with primitive stochastic functions and deterministic computation.\n",
    "\n",
    "For example, suppose we would like to model daily mean temperatures and cloud cover and how they interact with each other.\n",
    "\n",
    "Let's declare a function that will describe those interactions:\n",
    "  *  define a random variable `cloudy`, given by a draw from the Bernoulli distribution with param `0.3`; this will return either `0` or `1`, so then we parse it to a string; according to this line, 30% of the time it's cloudy, 70% it's sunny\n",
    "  *  define parameters of temperature distributions, i.e. mean and variance, for both cloudy and sunny weather: when it's sunny the mean is 18 degrees Celsius, with variance of 15 degrees, and when it's cloudy the mean equals 13 degrees with variance 15 degrees.\n",
    "  *  use the values to declare a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ and draw a sample\n",
    "  *  return the weather and temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather():\n",
    "    cloudy = torch.distributions.Bernoulli(0.3).sample()\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 13.0, 'sunny': 18.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = torch.distributions.Normal(mean_temp, scale_temp).rsample()\n",
    "    return cloudy, temp\n",
    "    \n",
    "\n",
    "for _ in range(3):\n",
    "    print(weather())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is independent of Pyro - it calls PyTorch `torch.distributions`. For a Pyro program we need to apply a few modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between `.sample()` and `.rsample()` - The Reparameterization Trick\n",
    "Imagine we have the following model and cost function which we want to minimize\n",
    "$$ x_i \\sim \\mathcal{N}(\\mu, \\sigma^2),\\  i = 1, \\ldots, N, \\ x_i \\ \\mathrm{iid} $$\n",
    "$$ \\mathcal{L}(\\mathbb{x}) = \\sum_i \\left(7 - x_i\\right)^2 $$\n",
    "What are the optimal values for $\\mu$ and $\\sigma$? Naturally these are 7 and 0 respectively. But imagine we do not know how to solve it straight through and we want to use an iterative technique that utilizes gradients.\n",
    "But how to calculate $ \\frac{\\partial \\mathcal{L}(\\mathbb{x})}{\\partial \\mu} $ and $ \\frac{\\partial \\mathcal{L}(\\mathbb{x})}{\\partial \\sigma} $ ? At each iteration we sample $N$ data points with $\\mathcal{N}(\\mu, \\sigma^2)$ (we need some initial values for the parameters) and then we loose the track of $\\mu$ and $\\sigma$. They do not appear in $\\mathcal{L}(\\mathbb{x})$. What if we reformulate the problem in such a manner?\n",
    "$$ x_i \\sim \\mathcal{N}(0, 1),\\  i = 1, \\ldots, N, \\ x_i \\ \\mathrm{iid} $$\n",
    "$$ \\mathcal{L}(\\mathbb{x}) = \\sum_i \\left(7 - (\\sigma x_i + \\mu) \\right)^2 $$\n",
    "The models are equivalent, but the gradients become tracktable! This is called `The Reparameterization Trick`. It is widely used in probabilistic machine learning, therefore it is already implemented in `torch`, we do not have to reparametrize manually.\n",
    "By calling `.sample()` we run the standard sampling process (without reparametrization).\n",
    "By calling `.rsample()` we run the sampling process with reparametrization - parametrs can be optimized.\n",
    "Not all distributions can be reparametrized! Check the `has_rsample` property before calling `.rsample()` (or go to documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pyro.sample`\n",
    "\n",
    "It is a Pyro primitive for a random variable. To use `pyro.sample` we need to introduce [`pyro.distributions`](http://docs.pyro.ai/en/1.3.0/distributions.html), which is a thin wrapper for `torch.distributions`, and `pyro.sample`, which replaces all calls of `.sample()` and `.rsample()` (it takes care of reparametrization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a sample from N(0, 1)\n",
    "loc = 0.  # mean\n",
    "scale = 1.  # variance\n",
    "x = pyro.sample(\"normal_sample\", pyro.distributions.Normal(loc, scale))\n",
    "print(f\"normal | sample: {x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a sample from B(alpha=2, beta=5)\n",
    "alpha = 2\n",
    "beta = 5\n",
    "x = pyro.sample(\"beta_sample\", pyro.distributions.Beta(alpha, beta))\n",
    "print(f\"beta | sample: {x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crucial difference here is that each sample is **named**. The backend of Pyro uses this names to uniquely identify sample statements and change their behavior at runtime, depending on how the enclosing stochastic function is used. This is how Pyro can implement various manipulations, that underlie inference algorithms.\n",
    "\n",
    "Here, we use a short name `dist` for `pyro.distributions`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather model rewritten as a Pyro program\n",
    "def weather():\n",
    "    cloudy = pyro.sample(\"cloudy\", dist.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 13.0, 'sunny': 18.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample(\"temp\", dist.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp\n",
    "\n",
    "\n",
    "for _ in range(3):\n",
    "    print(weather())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `weather()` is still a non-deterministic Python callable, that returns two random samples, but now invoked with `pyro.sample` - because of that Pyro allows us to access joint probability distribution over two named random variables: `cloudy` and `temp`, defining a probabilistic model that we can reason about using the techniques of probabilistic theory.\n",
    "\n",
    "A natural further step is to build off of the model. For example, ice cream sales depens heavily on the weather. Let's define a situation, where the expected sales depend on the cloudiness and the temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ice cream sales as depending on the weather\n",
    "def ice_cream_sales():\n",
    "    cloudy, temp = weather()\n",
    "    expected_sales = 200. if cloudy == \"sunny\" and temp > 20.0 else 50.\n",
    "    ice_cream = pyro.sample(\"ice_cream\", dist.Normal(expected_sales, 10.0))\n",
    "    return ice_cream\n",
    "\n",
    "for _ in range(3):\n",
    "    print(ice_cream_sales())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Pyro is embedded in Python, stochastic functions can contain complex deterministic Python - randomness can freely affect control flow! For example, recursive functions could be terminated nondeterministically, provided that we take care to use unique names of `pyro.sample`, such as in this geometric distribution counting the number of failures until the first success. Note, that each `pyro.sample` is created dynamically - we can have different numbers of named random variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of draws from Bernoulli distribution until success\n",
    "def geometric(p, t=None):\n",
    "    if t is None:\n",
    "        t = 0\n",
    "    x = pyro.sample(f\"x_{t}\", dist.Bernoulli(p))\n",
    "    if x.item() == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 + geometric(p, t + 1)\n",
    "\n",
    "print(geometric(0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also define stochastic functions that accept as input or produce as output other stochastic functions. Here, `normal_product` returns a product of two variables sampled from normal distribution with the same parameters, whereas `make_normal_normal` returns a product of two samples from a normal distribution, where the mean is sampled from $\\mathcal{N}(0, 1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_product(loc, scale):\n",
    "    z1 = pyro.sample(\"z1\", dist.Normal(loc, scale))\n",
    "    z2 = pyro.sample(\"z2\", dist.Normal(loc, scale))\n",
    "    y = z1 * z2\n",
    "    return y\n",
    "\n",
    "def make_normal_normal():\n",
    "    mu_latent = pyro.sample(\"mu_latent\", dist.Normal(0, 1))\n",
    "    fn = lambda scale: normal_product(mu_latent, scale)\n",
    "    return fn\n",
    "\n",
    "print(normal_product(0, 1))\n",
    "\n",
    "print(make_normal_normal()(1.))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a Pyro model of flipping a biased coin, using Bernoulli distribution. Set probability as $0.4$. Return the identifier and process in a loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flip_coin():\n",
    "    pass\n",
    "\n",
    "for _ in range(3):\n",
    "    print(\"heads\" if flip_coin() == 1 else \"tails\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a Pyro model of how much a person works (hourly employee). By default take Normal distribution with mean = 8, standard deviation = 1. Take into account the following factors:\n",
    "  *  being sick (decreases the number of hours in a day to 0)\n",
    "  *  unexpected event (decreases the number of hours by 2)\n",
    "  *  feeling lazy (leave work one hour earlier)\n",
    "  *  working overtime (stay two hours longer)\n",
    "  \n",
    "  Use Categorical distribution. Assume your own probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def working_hours():\n",
    "    pass\n",
    "\n",
    "for _ in range(3):\n",
    "    print(working_hours())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
